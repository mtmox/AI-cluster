1. Work on setup dir so that when i run ySETUP.py i can choose not to add cron
   jobs. The reason is that you dont want the cron job to execute on the computer
   that youre going to be using as the -frontend flag. It will mess up the logic
   because then your frontend computer will respond when the xSERVER.py file is
   turned on and you will start running a -backend instance flag along side 
   your -frontend instance.

   ***[DONE]***

2. Add in system-prompts to the front end so i can pick which system prompt I 
   want to send off with the payload. ***[DONE]***

3. Work on collecting all information from front end and then packaging it for
   the "in.chat.>" subject in the messages stream. All of our messages will go 
   through a collection function from the front end before they reach a subject.
   This collection will seperate them and divide them into the correct subjects.
   From here we will have a piece
   of code which determines which LLM we want to then go to. We are going to add
   in GPT, gemini, and anthropic later. We are starting with just Ollama first.
   Make code that first figures out which LLM to route to, then hands it off to
   the correct function which then takes our payload and structures it correctly
   for the LLM. ***[DONE]***

   Once we receive a response we then need to send it to "out.chat.>" subject in
   the messages stream, where it will then be picked up by a consumer and added
   to the correct threads in the correct conversation.
   ***[PROBLEM]*** - We need to figure out how to include the system prompt into
   the message. Right now its not being included in the chat endpoint queries or 
   we have to make a new modelfile for each system prompt and downloaded model
   combination.

4. We next need to create a way for the nodes to load balance themselves. I need 
   to think about this a bit first. The "in.chat.>" message structure is important
   because we dont want nodes consuming messages they cant process and then having 
   to republish them. The subject names should be ordered in a way where nodes 
   know before hand if they can even do any of the work within that subject. For 
   instance, we first need to convert every model name into a hash, otherwise we 
   are going to run into trouble with decimals and colons. Once we have created a 
   mapping each node will be able to take the models it has on its local drive
   and compare the hash the name generates against the hash in the subject line 
   "in.chat.hash.conversation.thread.>" where the conversation and thread arent 
   important the ".>" would go after hash. We will need to make durable pull 
   each node will spawn a durable pull consumer on every combination of 
   "in.chat.hash.>" that it has a corresponding local model for.

   ***[SOLUTION]*** -create a 10ms lag so that each node can only pull 1 msg at a
   time from the queue and after each pull it has to wait 10ms.
----------------------------------------------------------------------------------

5. Im not sure how but i need to make a standard error function. If we get an error 
   we need to call this function and then handle the error by writing it to a data-
   base. Down the line we can actually specialize some AI's with our specific code-
   base and when we get an error we can pass it to the AI's and see how they would 
   resolve the issue. I could fork the repo and make an AI controlled repo, ill need
   to familiarize myself with docker cause youll want a standard env where the AI's
   can pass changes to so they can be tested. IDK this part will be more difficult
   but its a possibility later.

6. Need to make storage databases to catch .log file output from nats, and we also 
   should be logging all program errors to a nats subject which we then write to a
   database. This will help figure out where problems are.   

----------------------------------------------------------------------------------

- Order models in dropdown by alphabetical.
- Fix problems with messages persisting after ive consumed them. ***[DONE]***

- I need to set up consumers unique consumers for each node following criteria of
  step 4. The consumers will pull messages based on headers from the queue. The 
  header will be what model is required to process the message, if the node has 
  that downloaded model then it can pull and process the message.
  ***[IMMEDIATE]***